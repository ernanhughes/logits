{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "import json\n",
    "import platform\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "class DeviceType(Enum):\n",
    "    CUDA = \"cuda\"\n",
    "    CPU = \"cpu\"\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model loading and inference\"\"\"\n",
    "    device_type: DeviceType\n",
    "    dtype: torch.dtype\n",
    "    device_map: Optional[Union[str, Dict]] = None\n",
    "\n",
    "def detect_device() -> ModelConfig:\n",
    "    \"\"\"\n",
    "    Detect the best available device and return appropriate configuration.\n",
    "    Returns: ModelConfig with optimal settings for the current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return ModelConfig(\n",
    "            device_type=DeviceType.CUDA,\n",
    "            dtype=torch.float16,  # Using float16 for CUDA by default\n",
    "            device_map=\"auto\"  # Let transformers handle multi-GPU setup\n",
    "        )\n",
    "    else:\n",
    "        return ModelConfig(\n",
    "            device_type=DeviceType.CPU,\n",
    "            dtype=torch.float32,  # CPU works better with float32\n",
    "            device_map=None\n",
    "        )\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_name: str,\n",
    "    config: Optional[ModelConfig] = None\n",
    ") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load model and tokenizer with optimal settings for the detected hardware.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the model\n",
    "        config: Optional ModelConfig, if None will auto-detect\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = detect_device()\n",
    "\n",
    "    print(f\"Loading model on {config.device_type.value} with {config.dtype}\")\n",
    "\n",
    "    # Load tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        padding_side=\"left\",  # Better for chat models\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Ensure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model with optimal settings for device\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": config.dtype,\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    "\n",
    "    if config.device_map is not None:\n",
    "        model_kwargs[\"device_map\"] = config.device_map\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **model_kwargs\n",
    "    )\n",
    "\n",
    "    # Handle device placement if no device_map\n",
    "    if config.device_map is None:\n",
    "        model = model.to(config.device_type.value)\n",
    "\n",
    "    return model, tokenizer, config\n",
    "\n",
    "def format_chat_prompt(messages: List[Dict[str, str]],\n",
    "                      tokenizer) -> str:\n",
    "    \"\"\"Format chat messages using model's template or fallback format\"\"\"\n",
    "    try:\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            return tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt = \"\"\n",
    "            for message in messages:\n",
    "                role = message['role']\n",
    "                content = message['content']\n",
    "                if role == 'system':\n",
    "                    formatted_prompt += f\"<|system|>\\n{content}\\n\"\n",
    "                elif role == 'user':\n",
    "                    formatted_prompt += f\"<|user|>\\n{content}\\n\"\n",
    "                elif role == 'assistant':\n",
    "                    formatted_prompt += f\"<|assistant|>\\n{content}\\n\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown role: {role}\")\n",
    "            return formatted_prompt + \"<|assistant|>\\n\"\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error formatting chat prompt: {e}\")\n",
    "\n",
    "def prepare_inputs(\n",
    "    text: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device_type: DeviceType\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Prepare model inputs with proper device placement\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Move tensors to appropriate device\n",
    "    device = device_type.value\n",
    "    inputs = {\n",
    "        k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "        for k, v in inputs.items()\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def get_chat_logprobs(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ModelConfig,\n",
    "    include_prompt: bool = True,\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 1.0,\n",
    "    batch_size: int = 1\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Get log probabilities for chat completion with optimal device handling.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries\n",
    "        model: Language model\n",
    "        tokenizer: Associated tokenizer\n",
    "        config: ModelConfig with device settings\n",
    "        include_prompt: Whether to include prompt tokens\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling parameter\n",
    "        batch_size: Batch size for processing\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains tokens, logprobs, and generation info\n",
    "    \"\"\"\n",
    "    # Ensure model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format prompt and prepare inputs\n",
    "    formatted_prompt = format_chat_prompt(messages, tokenizer)\n",
    "    inputs = prepare_inputs(formatted_prompt, tokenizer, config.device_type)\n",
    "\n",
    "    # Store offset mapping and remove from inputs\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "\n",
    "    # Context manager for different device types\n",
    "    if config.device_type == DeviceType.CUDA:\n",
    "        ctx = torch.cuda.amp.autocast()\n",
    "    else:\n",
    "        ctx = torch.no_grad()\n",
    "\n",
    "    with ctx:\n",
    "        try:\n",
    "            # Get model's output logits\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate probabilities and log probabilities\n",
    "            probs = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "            log_probs = torch.log(probs)\n",
    "\n",
    "            # Process tokens and logprobs\n",
    "            token_ids = inputs[\"input_ids\"][0]\n",
    "            token_logprobs = []\n",
    "            top_logprobs_list = []\n",
    "\n",
    "            # Batch process positions for efficiency\n",
    "            for batch_start in range(0, len(token_ids) - 1, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(token_ids) - 1)\n",
    "                batch_indices = range(batch_start, batch_end)\n",
    "\n",
    "                # Get next token ids for batch\n",
    "                next_token_ids = token_ids[batch_start + 1:batch_end + 1]\n",
    "\n",
    "                # Calculate logprobs for batch\n",
    "                batch_logprobs = log_probs[batch_indices, next_token_ids]\n",
    "                token_logprobs.extend(batch_logprobs.tolist())\n",
    "\n",
    "                # Get top alternative tokens for batch\n",
    "                top_values, top_indices = torch.topk(\n",
    "                    log_probs[batch_indices], 5, dim=-1\n",
    "                )\n",
    "\n",
    "                for pos_logprobs, pos_indices in zip(top_values, top_indices):\n",
    "                    top_logprobs = {\n",
    "                        tokenizer.decode([idx]): prob.item()\n",
    "                        for idx, prob in zip(pos_indices, pos_logprobs)\n",
    "                    }\n",
    "                    top_logprobs_list.append(top_logprobs)\n",
    "\n",
    "            # Add None for the last token\n",
    "            token_logprobs.append(None)\n",
    "            top_logprobs_list.append(None)\n",
    "\n",
    "            # Generate completion with appropriate settings\n",
    "            generation_config = {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": temperature > 0,\n",
    "                \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"return_dict_in_generate\": True,\n",
    "                \"output_scores\": True\n",
    "            }\n",
    "\n",
    "            # Add device-specific settings\n",
    "            if config.device_type == DeviceType.CUDA:\n",
    "                generation_config[\"use_cache\"] = True\n",
    "\n",
    "            gen_outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                **generation_config\n",
    "            )\n",
    "\n",
    "            # Process generation outputs\n",
    "            generated_ids = gen_outputs.sequences[0][len(token_ids):]\n",
    "            generated_tokens = tokenizer.convert_ids_to_tokens(generated_ids)\n",
    "            generated_text = tokenizer.decode(\n",
    "                generated_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Calculate generation logprobs\n",
    "            gen_logprobs, gen_top_logprobs = [], []\n",
    "            if hasattr(gen_outputs, \"scores\") and gen_outputs.scores:\n",
    "                for token_idx, token_scores in enumerate(gen_outputs.scores):\n",
    "                    if token_idx < len(generated_ids) - 1:\n",
    "                        next_token_id = generated_ids[token_idx + 1]\n",
    "                        probs = torch.nn.functional.softmax(token_scores[0], dim=-1)\n",
    "                        log_probs = torch.log(probs)\n",
    "\n",
    "                        # Get logprob for next token\n",
    "                        gen_logprobs.append(\n",
    "                            log_probs[next_token_id].item()\n",
    "                        )\n",
    "\n",
    "                        # Get top alternatives\n",
    "                        top_values, top_indices = torch.topk(log_probs, 5)\n",
    "                        top_logprobs = {\n",
    "                            tokenizer.decode([idx]): prob.item()\n",
    "                            for idx, prob in zip(top_indices, top_values)\n",
    "                        }\n",
    "                        gen_top_logprobs.append(top_logprobs)\n",
    "\n",
    "                gen_logprobs.append(None)\n",
    "                gen_top_logprobs.append(None)\n",
    "\n",
    "            # Combine results\n",
    "            all_tokens = (\n",
    "                tokenizer.convert_ids_to_tokens(token_ids) + generated_tokens\n",
    "            )\n",
    "            all_logprobs = token_logprobs + gen_logprobs\n",
    "            all_top_logprobs = top_logprobs_list + gen_top_logprobs\n",
    "\n",
    "            # Create final result\n",
    "            result = {\n",
    "                \"tokens\": all_tokens,\n",
    "                \"token_logprobs\": all_logprobs,\n",
    "                \"top_logprobs\": all_top_logprobs,\n",
    "                \"text\": tokenizer.decode(token_ids) + generated_text,\n",
    "                \"completion\": generated_text,\n",
    "                \"prompt\": formatted_prompt if include_prompt else None\n",
    "            }\n",
    "\n",
    "            # Remove prompt information if not requested\n",
    "            if not include_prompt:\n",
    "                prompt_length = len(token_ids)\n",
    "                for key in [\"tokens\", \"token_logprobs\", \"top_logprobs\"]:\n",
    "                    if result[key] is not None:\n",
    "                        result[key] = result[key][prompt_length:]\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during inference: {e}\")\n",
    "\n",
    "        finally:\n",
    "            # Clean up CUDA cache if needed\n",
    "            if config.device_type == DeviceType.CUDA:\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model on cpu\n",
      "Loading model on cpu with torch.float32\n"
     ]
    }
   ],
   "source": [
    "def example():\n",
    "    \"\"\"Example usage with device detection\"\"\"\n",
    "    try:\n",
    "        # Detect device and load model\n",
    "        config = detect_device()\n",
    "        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "        print(f\"\\nLoading model on {config.device_type.value}\")\n",
    "        model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "        # Example conversation\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the capital of France?\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Get completion with logprobs\n",
    "        result = get_chat_logprobs(\n",
    "            messages,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            config,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            batch_size=8  # Adjust based on available memory\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nPrompt:\", result[\"prompt\"])\n",
    "        print(\"\\nCompletion:\", result[\"completion\"])\n",
    "        print(\"\\nToken Details:\")\n",
    "        for token, logprob, top_logprobs in zip(\n",
    "            result[\"tokens\"],\n",
    "            result[\"token_logprobs\"],\n",
    "            result[\"top_logprobs\"]\n",
    "        ):\n",
    "            print(f\"\\nToken: {token}\")\n",
    "            print(f\"LogProb: {logprob}\")\n",
    "            if top_logprobs:\n",
    "                print(\"Top alternatives:\")\n",
    "                for token, prob in top_logprobs.items():\n",
    "                    print(f\"  {token}: {prob}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in example: {e}\")\n",
    "        raise\n",
    "\n",
    "example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
