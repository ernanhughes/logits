{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\logits\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "import json\n",
    "import platform\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "class DeviceType(Enum):\n",
    "    CUDA = \"cuda\"\n",
    "    MPS = \"mps\"\n",
    "    CPU = \"cpu\"\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model loading and inference\"\"\"\n",
    "    device_type: DeviceType\n",
    "    dtype: torch.dtype\n",
    "    device_map: Optional[Union[str, Dict]] = None\n",
    "\n",
    "def detect_device() -> ModelConfig:\n",
    "    \"\"\"\n",
    "    Detect the best available device and return appropriate configuration.\n",
    "    Returns: ModelConfig with optimal settings for the current hardware\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return ModelConfig(\n",
    "            device_type=DeviceType.CUDA,\n",
    "            dtype=torch.float16,  # Using float16 for CUDA by default\n",
    "            device_map=\"auto\"  # Let transformers handle multi-GPU setup\n",
    "        )\n",
    "    elif platform.processor() == 'arm' and torch.backends.mps.is_available():\n",
    "        return ModelConfig(\n",
    "            device_type=DeviceType.MPS,\n",
    "            dtype=torch.float16,  # MPS supports float16\n",
    "            device_map=None  # MPS doesn't use device_map\n",
    "        )\n",
    "    else:\n",
    "        return ModelConfig(\n",
    "            device_type=DeviceType.CPU,\n",
    "            dtype=torch.float32,  # CPU works better with float32\n",
    "            device_map=None\n",
    "        )\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_name: str,\n",
    "    config: Optional[ModelConfig] = None\n",
    ") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load model and tokenizer with optimal settings for the detected hardware.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the model\n",
    "        config: Optional ModelConfig, if None will auto-detect\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = detect_device()\n",
    "\n",
    "    print(f\"Loading model on {config.device_type.value} with {config.dtype}\")\n",
    "\n",
    "    # Load tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        padding_side=\"left\",  # Better for chat models\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Ensure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model with optimal settings for device\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": config.dtype,\n",
    "        \"trust_remote_code\": True\n",
    "    }\n",
    "\n",
    "    if config.device_map is not None:\n",
    "        model_kwargs[\"device_map\"] = config.device_map\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **model_kwargs\n",
    "    )\n",
    "\n",
    "    # Handle device placement if no device_map\n",
    "    if config.device_map is None:\n",
    "        model = model.to(config.device_type.value)\n",
    "\n",
    "    return model, tokenizer, config\n",
    "\n",
    "def format_chat_prompt(messages: List[Dict[str, str]],\n",
    "                      tokenizer) -> str:\n",
    "    \"\"\"Format chat messages using model's template or fallback format\"\"\"\n",
    "    try:\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            return tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt = \"\"\n",
    "            for message in messages:\n",
    "                role = message['role']\n",
    "                content = message['content']\n",
    "                if role == 'system':\n",
    "                    formatted_prompt += f\"<|system|>\\n{content}\\n\"\n",
    "                elif role == 'user':\n",
    "                    formatted_prompt += f\"<|user|>\\n{content}\\n\"\n",
    "                elif role == 'assistant':\n",
    "                    formatted_prompt += f\"<|assistant|>\\n{content}\\n\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown role: {role}\")\n",
    "            return formatted_prompt + \"<|assistant|>\\n\"\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error formatting chat prompt: {e}\")\n",
    "\n",
    "def prepare_inputs(\n",
    "    text: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device_type: DeviceType\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Prepare model inputs with proper device placement\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Move tensors to appropriate device\n",
    "    device = device_type.value\n",
    "    inputs = {\n",
    "        k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "        for k, v in inputs.items()\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def get_chat_logprobs(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    config: ModelConfig,\n",
    "    include_prompt: bool = True,\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 1.0,\n",
    "    batch_size: int = 1\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Get log probabilities for chat completion with optimal device handling.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries\n",
    "        model: Language model\n",
    "        tokenizer: Associated tokenizer\n",
    "        config: ModelConfig with device settings\n",
    "        include_prompt: Whether to include prompt tokens\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling parameter\n",
    "        batch_size: Batch size for processing\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains tokens, logprobs, and generation info\n",
    "    \"\"\"\n",
    "    # Ensure model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Format prompt and prepare inputs\n",
    "    formatted_prompt = format_chat_prompt(messages, tokenizer)\n",
    "    inputs = prepare_inputs(formatted_prompt, tokenizer, config.device_type)\n",
    "\n",
    "    # Store offset mapping and remove from inputs\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "\n",
    "    # Context manager for different device types\n",
    "    if config.device_type == DeviceType.CUDA:\n",
    "        ctx = torch.cuda.amp.autocast()\n",
    "    else:\n",
    "        ctx = torch.no_grad()\n",
    "\n",
    "    with ctx:\n",
    "        try:\n",
    "            # Get model's output logits\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate probabilities and log probabilities\n",
    "            probs = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "            log_probs = torch.log(probs)\n",
    "\n",
    "            # Process tokens and logprobs\n",
    "            token_ids = inputs[\"input_ids\"][0]\n",
    "            token_logprobs = []\n",
    "            top_logprobs_list = []\n",
    "\n",
    "            # Batch process positions for efficiency\n",
    "            for batch_start in range(0, len(token_ids) - 1, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(token_ids) - 1)\n",
    "                batch_indices = range(batch_start, batch_end)\n",
    "\n",
    "                # Get next token ids for batch\n",
    "                next_token_ids = token_ids[batch_start + 1:batch_end + 1]\n",
    "\n",
    "                # Calculate logprobs for batch\n",
    "                batch_logprobs = log_probs[batch_indices, next_token_ids]\n",
    "                token_logprobs.extend(batch_logprobs.tolist())\n",
    "\n",
    "                # Get top alternative tokens for batch\n",
    "                top_values, top_indices = torch.topk(\n",
    "                    log_probs[batch_indices], 5, dim=-1\n",
    "                )\n",
    "\n",
    "                for pos_logprobs, pos_indices in zip(top_values, top_indices):\n",
    "                    top_logprobs = {\n",
    "                        tokenizer.decode([idx]): prob.item()\n",
    "                        for idx, prob in zip(pos_indices, pos_logprobs)\n",
    "                    }\n",
    "                    top_logprobs_list.append(top_logprobs)\n",
    "\n",
    "            # Add None for the last token\n",
    "            token_logprobs.append(None)\n",
    "            top_logprobs_list.append(None)\n",
    "\n",
    "            # Generate completion with appropriate settings\n",
    "            generation_config = {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": temperature > 0,\n",
    "                \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"return_dict_in_generate\": True,\n",
    "                \"output_scores\": True\n",
    "            }\n",
    "\n",
    "            # Add device-specific settings\n",
    "            if config.device_type == DeviceType.CUDA:\n",
    "                generation_config[\"use_cache\"] = True\n",
    "\n",
    "            gen_outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                **generation_config\n",
    "            )\n",
    "\n",
    "            # Process generation outputs\n",
    "            generated_ids = gen_outputs.sequences[0][len(token_ids):]\n",
    "            generated_tokens = tokenizer.convert_ids_to_tokens(generated_ids)\n",
    "            generated_text = tokenizer.decode(\n",
    "                generated_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Calculate generation logprobs\n",
    "            gen_logprobs, gen_top_logprobs = [], []\n",
    "            if hasattr(gen_outputs, \"scores\") and gen_outputs.scores:\n",
    "                for token_idx, token_scores in enumerate(gen_outputs.scores):\n",
    "                    if token_idx < len(generated_ids) - 1:\n",
    "                        next_token_id = generated_ids[token_idx + 1]\n",
    "                        probs = torch.nn.functional.softmax(token_scores[0], dim=-1)\n",
    "                        log_probs = torch.log(probs)\n",
    "\n",
    "                        # Get logprob for next token\n",
    "                        gen_logprobs.append(\n",
    "                            log_probs[next_token_id].item()\n",
    "                        )\n",
    "\n",
    "                        # Get top alternatives\n",
    "                        top_values, top_indices = torch.topk(log_probs, 5)\n",
    "                        top_logprobs = {\n",
    "                            tokenizer.decode([idx]): prob.item()\n",
    "                            for idx, prob in zip(top_indices, top_values)\n",
    "                        }\n",
    "                        gen_top_logprobs.append(top_logprobs)\n",
    "\n",
    "                gen_logprobs.append(None)\n",
    "                gen_top_logprobs.append(None)\n",
    "\n",
    "            # Combine results\n",
    "            all_tokens = (\n",
    "                tokenizer.convert_ids_to_tokens(token_ids) + generated_tokens\n",
    "            )\n",
    "            all_logprobs = token_logprobs + gen_logprobs\n",
    "            all_top_logprobs = top_logprobs_list + gen_top_logprobs\n",
    "\n",
    "            # Create final result\n",
    "            result = {\n",
    "                \"tokens\": all_tokens,\n",
    "                \"token_logprobs\": all_logprobs,\n",
    "                \"top_logprobs\": all_top_logprobs,\n",
    "                \"text\": tokenizer.decode(token_ids) + generated_text,\n",
    "                \"completion\": generated_text,\n",
    "                \"prompt\": formatted_prompt if include_prompt else None\n",
    "            }\n",
    "\n",
    "            # Remove prompt information if not requested\n",
    "            if not include_prompt:\n",
    "                prompt_length = len(token_ids)\n",
    "                for key in [\"tokens\", \"token_logprobs\", \"top_logprobs\"]:\n",
    "                    if result[key] is not None:\n",
    "                        result[key] = result[key][prompt_length:]\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during inference: {e}\")\n",
    "\n",
    "        finally:\n",
    "            # Clean up CUDA cache if needed\n",
    "            if config.device_type == DeviceType.CUDA:\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model on cuda\n",
      "Loading model on cuda with torch.float16\n",
      "Error in example: Error formatting chat prompt: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error formatting chat prompt: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m, in \u001b[0;36mformat_chat_prompt\u001b[1;34m(messages, tokenizer)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapply_chat_template\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\projects\\logits\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1629\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n",
      "File \u001b[1;32md:\\projects\\logits\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1805\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[1;34m(self, chat_template, tools)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1805\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1808\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1809\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1810\u001b[0m         )\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in example: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mexample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mexample\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     {\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     }\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Get completion with logprobs\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mget_chat_logprobs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on available memory\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 178\u001b[0m, in \u001b[0;36mget_chat_logprobs\u001b[1;34m(messages, model, tokenizer, config, include_prompt, max_new_tokens, temperature, top_p, batch_size)\u001b[0m\n\u001b[0;32m    175\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Format prompt and prepare inputs\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mformat_chat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m inputs \u001b[38;5;241m=\u001b[39m prepare_inputs(formatted_prompt, tokenizer, config\u001b[38;5;241m.\u001b[39mdevice_type)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# Store offset mapping and remove from inputs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 121\u001b[0m, in \u001b[0;36mformat_chat_prompt\u001b[1;34m(messages, tokenizer)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m formatted_prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|assistant|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError formatting chat prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error formatting chat prompt: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "def example():\n",
    "    \"\"\"Example usage with device detection\"\"\"\n",
    "    try:\n",
    "        # Detect device and load model\n",
    "        config = detect_device()\n",
    "        model_name = 'E:/huggingface_models/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62/'\n",
    "#        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "        print(f\"\\nLoading model on {config.device_type.value}\")\n",
    "        model, tokenizer, config = load_model_and_tokenizer(model_name)\n",
    "\n",
    "        # Example conversation\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the capital of France?\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Get completion with logprobs\n",
    "        result = get_chat_logprobs(\n",
    "            messages,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            config,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            batch_size=8  # Adjust based on available memory\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nPrompt:\", result[\"prompt\"])\n",
    "        print(\"\\nCompletion:\", result[\"completion\"])\n",
    "        print(\"\\nToken Details:\")\n",
    "        for token, logprob, top_logprobs in zip(\n",
    "            result[\"tokens\"],\n",
    "            result[\"token_logprobs\"],\n",
    "            result[\"top_logprobs\"]\n",
    "        ):\n",
    "            print(f\"\\nToken: {token}\")\n",
    "            print(f\"LogProb: {logprob}\")\n",
    "            if top_logprobs:\n",
    "                print(\"Top alternatives:\")\n",
    "                for token, prob in top_logprobs.items():\n",
    "                    print(f\"  {token}: {prob}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in example: {e}\")\n",
    "        raise\n",
    "\n",
    "example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
