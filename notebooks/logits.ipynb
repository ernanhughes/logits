{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\logits\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: What is the capital of France? Paris is the capital of France. It is also the most populous city in France and is the country's largest urban area. The city is a global center for art, fashion, culture, and cuisine, and\n",
      "Probability: 7.6594e-11\n",
      "\n",
      "Result: What is the capital of France? What is the capital of France? What is the capital of France? What is the capital of France?\n",
      "A. Paris\n",
      "B. Lyon\n",
      "C. Bordeaux\n",
      "D. Marseille\n",
      "\n",
      "The correct answer is A\n",
      "Probability: 0.0000e+00\n",
      "\n",
      "Result: What is the capital of France? Paris.\n",
      "What is the capital of Australia? Canberra.\n",
      "What is the capital of Germany? Berlin.\n",
      "What is the capital of South Africa? Cape Town.\n",
      "What is the capital of Italy? Rome.\n",
      "What is\n",
      "Probability: 0.0000e+00\n",
      "\n",
      "Result: What is the capital of France? Paris.\n",
      "What is the capital of England? London.\n",
      "What is the capital of Italy? Rome.\n",
      "What is the capital of Germany? Berlin.\n",
      "What is the capital of Japan? Tokyo.\n",
      "What is the capital\n",
      "Probability: 0.0000e+00\n",
      "\n",
      "Result: What is the capital of France? Paris\n",
      "What is the capital of France?\n",
      "Answer: Paris\n",
      "Explanation: The capital of France is Paris. It is the most populous city in France and the country's largest metropolitan area. Paris has been a\n",
      "Probability: 0.0000e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def generate_results_with_probabilities(model_name, input_text, num_results=5, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates a list of results and their probabilities using a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the Hugging Face model.\n",
    "        input_text (str): The input text.\n",
    "        num_results (int): The number of results to generate.\n",
    "        max_length (int): The maximum length of the generated sequences.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the generated text and its probability.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    max_context_length = 1024  # Set your desired context length\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\",\n",
    "        truncation=True, max_length=max_context_length).to(device)\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            num_return_sequences=num_results,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_length=max_length,\n",
    "            do_sample=True, # enables sampling\n",
    "            temperature=0.7, # adjust for more/less random results\n",
    "        )\n",
    "        generated_ids = outputs.sequences\n",
    "        transition_scores = outputs.scores\n",
    "\n",
    "        for i in range(num_results):\n",
    "            generated_sequence = generated_ids[i]\n",
    "            probabilities = []\n",
    "            start_index = len(inputs['input_ids'][0])\n",
    "\n",
    "            for j in range(len(transition_scores)):\n",
    "                logits = transition_scores[j]\n",
    "                chosen_token_id = generated_sequence[start_index + j]\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                chosen_token_prob = probs[0, chosen_token_id].item()\n",
    "                probabilities.append(chosen_token_prob)\n",
    "\n",
    "            generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "\n",
    "            # calculate the product of all probabilities. This is a very rough estimate of the probability of the entire sequence.\n",
    "            sequence_probability = 1.0\n",
    "            for prob in probabilities:\n",
    "                sequence_probability *= prob\n",
    "\n",
    "            results.append({\n",
    "                \"generated_text\": generated_text,\n",
    "                \"sequence_probability\": sequence_probability,\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Replace with your model\n",
    "input_text = \"What is the capital of France?\"\n",
    "num_results = 5 # number of different results to return.\n",
    "\n",
    "results = generate_results_with_probabilities(model_name, input_text, num_results)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Result: {result['generated_text']}\")\n",
    "    print(f\"Probability: {result['sequence_probability']:.4e}\\n\") # format the probablity to a scientific notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Why is the sky blue?\n",
      "Generated text: Why is the sky blue? (a) The sky is blue because the earth is surrounded by a blue gas called the atmosphere. (b) The sky is blue because the blue light from the sun is scattered in all directions by the tiny molecules\n",
      "Probabilities: [0.06012797728180885, 0.008771602064371109, 0.7183493971824646, 0.2694494426250458, 0.941425085067749, 0.530074954032898, 1.0, 1.0, 0.3310956656932831, 0.03848743066191673, 0.6673692464828491, 0.2657732665538788, 1.0, 0.947877824306488, 0.4544859528541565, 0.3774600625038147, 0.5188309550285339, 0.19763371348381042, 0.8107373118400574, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7413054704666138, 0.034919656813144684, 0.7732970714569092, 0.23932836949825287, 1.0, 1.0, 0.8248728513717651, 1.0, 0.4079786539077759, 1.0, 1.0, 1.0, 1.0, 0.9180664420127869, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Replace with your model's identifier\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input_text = \"Why is the sky blue?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, output_scores=True, return_dict_in_generate=True, max_length=50) # Adjust max_length\n",
    "    generated_ids = outputs.sequences\n",
    "    transition_scores = outputs.scores # This is a tuple of tensors\n",
    "\n",
    "probabilities = []\n",
    "for i in range(len(transition_scores)):\n",
    "    # grab the logits for the next token\n",
    "    logits = transition_scores[i]\n",
    "    # grab the token id that was actually chosen at that step.\n",
    "    chosen_token_id = generated_ids[0, len(inputs['input_ids'][0]) + i]\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    # Get the probability of the chosen token\n",
    "    chosen_token_prob = probs[0, chosen_token_id].item()\n",
    "    probabilities.append(chosen_token_prob)\n",
    "\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "next_token = tokenizer.decode(generated_ids[0, -1], skip_special_tokens=True)\n",
    "\n",
    "results = {\n",
    "    \"generated_text\": generated_text,\n",
    "    \"token_probabilities\": probabilities,\n",
    "}\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
